{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import relevant libraries and read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Set to raise an error for chained assignment\n",
    "pd.set_option('mode.chained_assignment', 'raise')\n",
    "\n",
    "# Read the data as pandas DataFrame\n",
    "data = pd.read_csv('../../data/prostate.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the data into train/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y: response variables, train_label: checks whether it is for training or not\n",
    "Y = data.pop('lpsa')\n",
    "train_label = data.pop('train')\n",
    "\n",
    "# Divide the data into train/test\n",
    "train_X = data.loc[train_label == 'T', :].copy()\n",
    "train_Y = Y[train_label == 'T'].copy()\n",
    "test_X = data.loc[train_label == 'F', :].copy()\n",
    "test_Y = Y[train_label == 'F'].copy()\n",
    "'''\n",
    "# Divide into train and test input Randomly!\n",
    "train_X = data.sample(n=67, random_state=5, axis=0)\n",
    "train_Y = Y[train_X.index].copy()\n",
    "\n",
    "test_X = data.drop(train_X.index)\n",
    "test_Y = Y[test_X.index].copy()\n",
    "'''\n",
    "\n",
    "# Feature names\n",
    "features = data.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize the training predictor values and fit the linear mode into the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                   lpsa   R-squared:                       0.694\n",
      "Model:                            OLS   Adj. R-squared:                  0.652\n",
      "Method:                 Least Squares   F-statistic:                     16.47\n",
      "Date:                Tue, 02 Feb 2021   Prob (F-statistic):           2.04e-12\n",
      "Time:                        22:50:16   Log-Likelihood:                -67.505\n",
      "No. Observations:                  67   AIC:                             153.0\n",
      "Df Residuals:                      58   BIC:                             172.9\n",
      "Df Model:                           8                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      2.4523      0.087     28.182      0.000       2.278       2.627\n",
      "lcavol         0.7110      0.133      5.366      0.000       0.446       0.976\n",
      "lweight        0.2905      0.106      2.751      0.008       0.079       0.502\n",
      "age           -0.1415      0.101     -1.396      0.168      -0.344       0.061\n",
      "lbph           0.2104      0.102      2.056      0.044       0.006       0.415\n",
      "svi            0.3073      0.124      2.469      0.017       0.058       0.556\n",
      "lcp           -0.2868      0.154     -1.867      0.067      -0.594       0.021\n",
      "gleason       -0.0208      0.142     -0.147      0.884      -0.304       0.263\n",
      "pgg45          0.2753      0.158      1.738      0.088      -0.042       0.592\n",
      "==============================================================================\n",
      "Omnibus:                        0.825   Durbin-Watson:                   1.690\n",
      "Prob(Omnibus):                  0.662   Jarque-Bera (JB):                0.389\n",
      "Skew:                          -0.164   Prob(JB):                        0.823\n",
      "Kurtosis:                       3.178   Cond. No.                         4.44\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# Normalize the input\n",
    "scaler = preprocessing.StandardScaler().fit(train_X)\n",
    "train_X.loc[:, features] = scaler.transform(train_X)\n",
    "\n",
    "# add 1's for the intercept\n",
    "train_X = sm.add_constant(train_X)\n",
    "train_X.rename(columns={'const': 'Intercept'}, inplace=True)\n",
    "\n",
    "# Linear Regression Result\n",
    "ols = sm.OLS(train_Y, train_X)\n",
    "ols_res = ols.fit()\n",
    "print(ols_res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compared to the base model (average of training response), the linear model reduces 50.67% of test Mean Squared Error (MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE from Linear Model: 0.5213\n",
      "MSE from base Model: 1.0567\n",
      "MSE decreased by 50.67% with the linear model\n"
     ]
    }
   ],
   "source": [
    "# Scale test input with mean, and variance from train input\n",
    "test_X.loc[:, features] = scaler.transform(test_X)\n",
    "test_X = sm.add_constant(test_X)\n",
    "test_X.rename(columns={'const': 'Intercept'}, inplace=True)\n",
    "\n",
    "# Predicted values and MSE for the test input\n",
    "predicted_Y = ols_res.predict(test_X)\n",
    "test_MSE = sum((predicted_Y - test_Y)**2) / test_Y.shape[0]\n",
    "\n",
    "# As a base error, we use the average of the train_Y\n",
    "base_MSE = sum((np.mean(train_Y) - test_Y)**2) / test_Y.shape[0]\n",
    "\n",
    "print(f'MSE from Linear Model: {test_MSE:.4f}')\n",
    "print(f'MSE from base Model: {base_MSE:.4f}')\n",
    "print(f'MSE decreased by {(base_MSE - test_MSE)/base_MSE*100:.2f}% with the linear model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We have followed the ESL so far, but we may have a better result if we introduce some categorical features (age, svi, gleason)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 additional binary variables are added + 1 svi (un-normalized)\n",
    "- 'age_lo', 'age_mid', 'age_hi'\n",
    "- 'gleason_6', 'gleason_7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We make dummy variables to handle categorical features\n",
    "# age: low (<=55),\n",
    "#      mid (55< <=65)\n",
    "#      hi  (>65)\n",
    "\n",
    "# svi: 0 vs 1\n",
    "\n",
    "# gleason: 6 vs 7 vs {8, 9}\n",
    "\n",
    "new_data = data.copy()\n",
    "\n",
    "# age dummies\n",
    "new_data['age_lo'] = (data.loc[:, 'age'] <= 55)\n",
    "new_data['age_mid'] = (55 < data.loc[:, 'age']) & (data.loc[:, 'age'] <= 65)\n",
    "new_data['age_hi'] = (data.loc[:, 'age'] > 65)\n",
    "\n",
    "new_data.loc[:, 'age_lo'] = (new_data.loc[:, 'age_lo']).astype(int)\n",
    "new_data.loc[:, 'age_mid'] = (new_data.loc[:, 'age_mid']).astype(int)\n",
    "new_data.loc[:, 'age_hi'] = (new_data.loc[:, 'age_hi']).astype(int)\n",
    "new_data.pop('age')\n",
    "\n",
    "# svi -> just leave it without normalizing\n",
    "\n",
    "# gleason dummies\n",
    "new_data['gleason_6'] = (data.loc[:, 'gleason'] == 6)\n",
    "new_data['gleason_7'] = (data.loc[:, 'gleason'] == 7)\n",
    "\n",
    "new_data.loc[:, 'gleason_6'] = (new_data.loc[:, 'gleason_6']).astype(int)\n",
    "new_data.loc[:, 'gleason_7'] = (new_data.loc[:, 'gleason_7']).astype(int)\n",
    "new_data.pop('gleason');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The upper part follows the same split of train/test data as in ESL, whereas the lower part splits the data randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Divide into train and test input Randomly!\\nnew_train_X = new_data.sample(n=67, random_state=5, axis=0)\\nnew_train_Y = Y[new_train_X.index].copy()\\n\\nnew_test_X = new_data.drop(new_train_X.index)\\nnew_test_Y = Y[new_test_X.index].copy()\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Divide into train and test input (Same as in ESL)\n",
    "new_train_X = new_data.loc[train_label == 'T', :].copy()\n",
    "new_train_Y = Y[train_label == 'T'].copy()\n",
    "\n",
    "new_test_X = new_data.loc[train_label == 'F', :].copy()\n",
    "new_test_Y = Y[train_label == 'F'].copy()\n",
    "'''\n",
    "# Divide into train and test input Randomly!\n",
    "new_train_X = new_data.sample(n=67, random_state=5, axis=0)\n",
    "new_train_Y = Y[new_train_X.index].copy()\n",
    "\n",
    "new_test_X = new_data.drop(new_train_X.index)\n",
    "new_test_Y = Y[new_test_X.index].copy()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize only the qunatitative variables, and fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                   lpsa   R-squared:                       0.698\n",
      "Model:                            OLS   Adj. R-squared:                  0.644\n",
      "Method:                 Least Squares   F-statistic:                     12.96\n",
      "Date:                Tue, 02 Feb 2021   Prob (F-statistic):           2.43e-11\n",
      "Time:                        22:50:17   Log-Likelihood:                -67.070\n",
      "No. Observations:                  67   AIC:                             156.1\n",
      "Df Residuals:                      56   BIC:                             180.4\n",
      "Df Model:                          10                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      1.6013      0.331      4.843      0.000       0.939       2.264\n",
      "lcavol         0.6712      0.138      4.879      0.000       0.396       0.947\n",
      "lweight        0.2991      0.107      2.795      0.007       0.085       0.513\n",
      "lbph           0.1664      0.105      1.581      0.120      -0.044       0.377\n",
      "svi            0.6224      0.312      1.995      0.051      -0.003       1.247\n",
      "lcp           -0.2475      0.156     -1.583      0.119      -0.561       0.066\n",
      "pgg45          0.2046      0.153      1.335      0.187      -0.103       0.512\n",
      "age_lo         0.7620      0.291      2.622      0.011       0.180       1.344\n",
      "age_mid        0.4738      0.172      2.748      0.008       0.128       0.819\n",
      "age_hi         0.3655      0.167      2.185      0.033       0.030       0.701\n",
      "gleason_6      0.1190      0.530      0.225      0.823      -0.943       1.181\n",
      "gleason_7      0.3972      0.423      0.938      0.352      -0.451       1.245\n",
      "==============================================================================\n",
      "Omnibus:                        0.040   Durbin-Watson:                   1.614\n",
      "Prob(Omnibus):                  0.980   Jarque-Bera (JB):                0.109\n",
      "Skew:                          -0.054   Prob(JB):                        0.947\n",
      "Kurtosis:                       2.834   Cond. No.                     2.08e+16\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 4.07e-31. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n"
     ]
    }
   ],
   "source": [
    "# Normalize except age, svi, gleason variables\n",
    "quant_features = ['lcavol', 'lweight', 'lbph', 'lcp', 'pgg45']\n",
    "normalizer = preprocessing.StandardScaler().fit(new_train_X.loc[:, quant_features])\n",
    "new_train_X.loc[:, quant_features] = normalizer.transform(new_train_X.loc[:, quant_features])\n",
    "\n",
    "# add 1's for the intercept\n",
    "new_train_X = sm.add_constant(new_train_X)\n",
    "new_train_X.rename(columns={'const': 'Intercept'}, inplace=True)\n",
    "\n",
    "# Linear Regression Result\n",
    "new_ols = sm.OLS(new_train_Y, new_train_X)\n",
    "new_ols_res = new_ols.fit()\n",
    "print(new_ols_res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Different split gave lower MSE, but to justfy that introducing the categorical variables is the better model, you need to repeat random split and average the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE from New Linear Model: 0.4692\n",
      "MSE from base Model: 1.0567\n",
      "MSE decreased by 55.60% with the new linear model\n"
     ]
    }
   ],
   "source": [
    "# Scale test input with mean, and variance from train input\n",
    "new_test_X.loc[:, quant_features] = normalizer.transform(new_test_X.loc[:, quant_features])\n",
    "new_test_X = sm.add_constant(new_test_X)\n",
    "new_test_X.rename(columns={'const': 'Intercept'}, inplace=True)\n",
    "\n",
    "# Predicted values and MSE for the test input\n",
    "new_predicted_Y = new_ols_res.predict(new_test_X)\n",
    "new_test_MSE = sum((new_predicted_Y - new_test_Y)**2) / new_test_Y.shape[0]\n",
    "\n",
    "# As a base error, we use the average of the training response\n",
    "base_MSE = sum((np.mean(new_train_Y) - new_test_Y)**2) / new_test_Y.shape[0]\n",
    "\n",
    "print(f'MSE from New Linear Model: {new_test_MSE:.4f}')\n",
    "print(f'MSE from base Model: {base_MSE:.4f}')\n",
    "print(f'MSE decreased by {(base_MSE - new_test_MSE)/base_MSE*100:.2f}% with the new linear model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
